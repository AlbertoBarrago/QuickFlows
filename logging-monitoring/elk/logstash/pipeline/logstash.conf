# Logstash Pipeline Configuration
# This pipeline processes logs from Filebeat and other sources

input {
  # Receive logs from Filebeat
  beats {
    port => 5044
    ssl => false
  }
  
  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json
  }
  
  # HTTP input for webhook-based logs
  http {
    port => 8080
    codec => json
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  # Add timestamp if missing
  if ![timestamp] {
    date {
      match => [ "@timestamp", "ISO8601" ]
      target => "timestamp"
    }
  }
  
  # Grok filter for parsing unstructured logs
  if [log_source] == "nginx" and [log_type] == "access" {
    grok {
      match => { "message" => "%{IPORHOST:remote_ip} - %{DATA:user_name} \[%{HTTPDATE:access_time}\] \"%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} \"%{DATA:referrer}\" \"%{DATA:agent}\"" }
    }
    date {
      match => [ "access_time", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    mutate {
      convert => {
        "response_code" => "integer"
        "body_sent_bytes" => "integer"
      }
    }
  }
  
  # Parse application logs
  if [log_source] == "application" {
    grok {
      match => { "message" => "(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}) \[%{LOGLEVEL:log_level}\] %{GREEDYDATA:log_message}" }
    }
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
  }
  
  # Enrich logs with geolocation data for IP addresses
  if [remote_ip] {
    geoip {
      source => "remote_ip"
      target => "geoip"
    }
  }
  
  # Drop debug logs in production
  if [environment] == "production" and [log_level] == "DEBUG" {
    drop {}
  }
  
  # Add tags for easier searching
  if [response_code] and [response_code] >= 400 {
    mutate {
      add_tag => ["error"]
    }
  }
  
  if [response_code] and [response_code] >= 500 {
    mutate {
      add_tag => ["critical"]
    }
  }
}

output {
  # Send all logs to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[log_source]}-%{+YYYY.MM.dd}"
    document_type => "_doc"
    # For version compatibility
    ecs_compatibility => disabled
  }
  
  # Send critical errors to a separate index
  if "critical" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "critical-errors-%{+YYYY.MM.dd}"
      document_type => "_doc"
      ecs_compatibility => disabled
    }
  }
  
  # Optional: Output to stdout for debugging
  # stdout { codec => rubydebug }
}